{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c0f394",
   "metadata": {},
   "source": [
    "# Exercise 3 - Review questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330e2ec",
   "metadata": {},
   "source": [
    "### a) Why do we have a gradient ascent in the case of logistic regression while we had a gradient descent with linear regression ? Can we convert the gradient ascent of logistic regression into a gradient descent ? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108d69de",
   "metadata": {},
   "source": [
    "Logistic regression use gradient ascent because it aims to maximize the likelihood function (which is the cost function). Therefore we move in the direction of the gradient.\n",
    "Yes, we can convert ascent to descent by changing the objective from a maximization problem to a minimization problem. It is equivalent to take the negative of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9000c86",
   "metadata": {},
   "source": [
    "### b) Assuming a logistic regression with a linear decision boundary taking as input samples in two dimensions (x1; x2), in which case do we get 0.5 as output of the classification system ? Express your answer with an equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adfff97",
   "metadata": {},
   "source": [
    "According to the logistic regression formula, we get 0.5 when the input is 0.\n",
    "$0.5 = 1/(1+exp(-\\theta^T*x))$ => $\\theta^T*x$ is equal to 0.\n",
    "The equation of the decision boundary is:\n",
    "$\\theta_0 + \\theta_1 x_0 + \\theta_2 x_1 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d782c",
   "metadata": {},
   "source": [
    "### c) What is the computational trick to avoid numerical problems in the computation of J(theta) for the logistic regression ? In which situations (for what type of inputs) do we risk to observe such numerical problems ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e405c9",
   "metadata": {},
   "source": [
    "With the cost function $J(\\theta)$:\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
    "$$\n",
    "We can have numerical problems when $h_\\theta(x^{(i)})$ is close to 0 or 1. In this case, the logarithm of $h_\\theta(x^{(i)})$ or $1 - h_\\theta(x^{(i)})$ will be close to $-\\infty$ and can lead to numerical instability. \n",
    "The solution is to add a small value $\\epsilon$ to $h_\\theta(x^{(i)})$ and $1 - h_\\theta(x^{(i)})$. It gives:\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)}) + \\epsilon) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}) + \\epsilon) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1657c670",
   "metadata": {},
   "source": [
    "### d) A logistic regression can classify between 2 classes. How can we build a multi-class (with K classes) system with logistic regression ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c9e92c",
   "metadata": {},
   "source": [
    "It can be done with the one-vs-rest method. \n",
    "OvR \"involves splitting the multi-class dataset into multiple binary classification problems.\" (source: https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)\n",
    "Principle:\n",
    "For each class k (k=1,2,...,K):\n",
    "- Treat class k as the positive class.\n",
    "- Treat all other classes as the negative class.\n",
    "- Train a binary classifier on this binary representation.\n",
    "- Repeat for each class in the dataset.\n",
    "\n",
    "After this process, you will have K binary classifiers (and K models)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
