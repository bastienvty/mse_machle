{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08113539-a4cf-4930-9503-c5f2d2ee3cd6",
   "metadata": {},
   "source": [
    "# Gradient Boosting with Random Forests and related topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e198f-2a23-412d-917e-88cbb53e71ef",
   "metadata": {},
   "source": [
    "In this last notebook, we will explore the Gradient Boosting technique for classification. This technique combines the principles of gradient boosting and random forests by sequentially fitting decision trees to the errors of the previous trees, improving model accuracy while maintaining the benefits of ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b806e144-7934-41c0-a55f-d8e600f9d79c",
   "metadata": {},
   "source": [
    "### Import and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c1e3cf-3ae1-4a76-bd97-2541198c2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS PACKAGES\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5528bcb-b583-442b-aa60-d83ed1db5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), 'EEG_mouse_data.csv')\n",
    "data_mice = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4929e82-d5de-4e3a-b081-af667569e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_0_1(data):\n",
    "    # Calculate the minimum and maximum values for each column (feature)\n",
    "    min_vals = data.min(axis=0)\n",
    "    max_vals = data.max(axis=0)\n",
    "    #print(max_vals)\n",
    "    \n",
    "    # Perform min-max scaling\n",
    "    normalized_data = (data - min_vals) / (max_vals - min_vals)\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "#CREATE X AND Y FROM DATAFRAME\n",
    "X = np.vstack([data_mice.iloc[i, 2:] for i in np.arange(len(data_mice))])\n",
    "X = norm_0_1(X)\n",
    "Y = data_mice['state'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa023fb-eead-4ce3-9885-a4a8c98c9c74",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6328def-1f62-4c22-9188-2a6bbab0a8d1",
   "metadata": {},
   "source": [
    "This time, the hyperparameters have already been fine-tuned, so you don't need to adjust them yourself. Simply execute the cell to train the model; it might take 1 or 2 minutes.\n",
    "\n",
    "Q3.1: Two additional hyperparameters were added compared to the RandomForestClassifier. What are these hyperparameters, and what roles do they play?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd8fe373-3c56-4c78-a994-eb145c93883f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting model trained\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define hyperparameters for Gradient Boosting\n",
    "hyperparameters_gb = {\n",
    "    'n_estimators': 300,\n",
    "    'learning_rate': 0.05, \n",
    "    'max_depth': 4,         \n",
    "    'subsample': 0.5,\n",
    "    'min_samples_leaf': 15,\n",
    "    'max_features': 20,\n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting classifier with the specified hyperparameters\n",
    "gb_classifier = GradientBoostingClassifier(**hyperparameters_gb,random_state=0)\n",
    "\n",
    "# Fit the model on the training data\n",
    "gb_classifier.fit(X_train, Y_train)\n",
    "print('Gradient Boosting model trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4712b3-6f1c-4473-995b-44855b68992a",
   "metadata": {},
   "source": [
    "### FINAL MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ea6a5-61f3-4426-8217-828afa6f5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "train_prediction = gb_classifier.predict(X_train)\n",
    "test_prediction = gb_classifier.predict(X_test)\n",
    "test_prediction\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(Y_train, train_prediction)\n",
    "test_accuracy = accuracy_score(Y_test, test_prediction)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Confusion matrix for the training set\n",
    "cm_train = confusion_matrix(Y_train, train_prediction, normalize='true')\n",
    "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=gb_classifier.classes_)\n",
    "_=disp_train.plot(ax=axs[0], values_format=\".2f\")\n",
    "_=axs[0].set_title('Confusion Matrix (Training Set)')\n",
    "# Confusion matrix for the test set\n",
    "cm_test = confusion_matrix(Y_test, test_prediction, normalize='true')\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=gb_classifier.classes_)\n",
    "_=disp_test.plot(ax=axs[1], values_format=\".2f\")\n",
    "_=axs[1].set_title('Confusion Matrix (Test Set)')\n",
    "\n",
    "# Display accuracy for both sets\n",
    "print('Accuracy on Training Set:', train_accuracy)\n",
    "print('Accuracy on Test Set:', test_accuracy)\n",
    "\n",
    "# Classification report for the training set\n",
    "classification_metrics_train = classification_report(Y_train, train_prediction, output_dict=True)\n",
    "report_df_train = pd.DataFrame(classification_metrics_train).transpose()\n",
    "print(\"Classification Report for Training Set:\")\n",
    "print(report_df_train)\n",
    "\n",
    "# Classification report for the test set\n",
    "classification_metrics_test = classification_report(Y_test, test_prediction, output_dict=True)\n",
    "report_df_test = pd.DataFrame(classification_metrics_test).transpose()\n",
    "print(\"Classification Report for Test Set:\")\n",
    "print(report_df_test)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ebee96-3a7a-4d78-a275-f07e36f1a427",
   "metadata": {},
   "source": [
    "The cell above summarizes performance of the GradientBoostingClassifier.\n",
    "\n",
    "Q3.2: Comment the results. Compare these results with the ones obtained with the RandomForestClassifier. Compare more specifically the precision, the recall and the f1-score of the 'r' class obtained with GradientBoostingClassifier and RandomForestClassifier. What are your conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ddfed-4409-49ea-ad3f-00a4d389a9bf",
   "metadata": {},
   "source": [
    "NOTE: It is interesting to note that, in a real-world scenario, performances for this task could be further improved. For example, you could apply more advanced feature engineering techniques, or even incorporate other sensors, such as an electromyograph (EMG) for recording the electrical activity in skeletal muscles. Nevertheless, these aspects are beyond the scope of this practical work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
