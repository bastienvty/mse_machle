{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"X_wPegydbafH"},"source":["# Computing features to train a MLP\n","This notebook will guide you through the use of the `keras` package to train a multilayer perceptron for handwritten digits classification. You are going to computing the histogram of gradients from the images in the `mnist` dataset (LeCun et al. 1998)"]},{"cell_type":"markdown","metadata":{"id":"61oGWRyMbafJ"},"source":["## Loading the packages"]},{"cell_type":"code","metadata":{"id":"duI9Ar-8bafS"},"source":["%pip install tensorflow --upgrade\n","%pip install keras --upgrade\n","\n","import numpy as np\n","from matplotlib import pyplot as pl\n","\n","from skimage.feature import hog\n","from skimage import data, color, exposure\n","\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Dropout\n","from tensorflow.keras.optimizers import RMSprop\n","from keras.utils import np_utils\n","from sklearn import metrics as me\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KpPmUqiRbafz"},"source":["Load the `mnist` dataset and normalize in the range [0, 1]"]},{"cell_type":"code","metadata":{"id":"verKKFaFbaf6"},"source":["(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","# Split the training set into a training set and a validation set\n","X_val = X_train[0:10000,:,:]\n","X_train = X_train[10000:,:,:]\n","y_val = y_train[0:10000]\n","y_train = y_train[10000:]\n","\n","n_train, height, width = X_train.shape\n","n_val, _, _ = X_val.shape\n","n_test, _, _ = X_test.shape\n","\n","X_train = X_train.reshape(n_train, height, width, 1).astype('float32')\n","X_val = X_val.reshape(n_val, height, width, 1).astype('float32')\n","X_test = X_test.reshape(n_test, height, width, 1).astype('float32')\n","\n","X_train /= 255.0\n","X_val /= 255.0\n","X_test /= 255.0\n","\n","n_classes = 10\n","\n","print(n_train, 'train samples')\n","print(n_val, 'validation samples')\n","print(n_test, 'test samples')\n","\n","# convert class vectors to binary class matrices\n","Y_train = np_utils.to_categorical(y_train, n_classes)\n","Y_val = np_utils.to_categorical(y_val, n_classes)\n","Y_test = np_utils.to_categorical(y_test, n_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TazeB7LLbagE"},"source":["### An example of how to compute the histogram of gradients\n","Let's compute the HOG for one image in the test dataset"]},{"cell_type":"code","metadata":{"id":"cL-609oKbagI"},"source":["n_orientations = 8\n","pix_p_cell = 4\n","hog_size = int(height * width * n_orientations / (pix_p_cell * pix_p_cell))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2DA2Da_bage"},"source":["fd, hog_image = hog(X_test[0,:,:,0], orientations=n_orientations, pixels_per_cell=(pix_p_cell, pix_p_cell), cells_per_block=(1, 1), visualize=True)\n","hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 0.02))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s2Ow4uJtbago"},"source":["Show the image and the corresponding gradients"]},{"cell_type":"code","metadata":{"id":"83RZeljKbagu"},"source":["fig, (ax1, ax2) = pl.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n","ax1.axis('off')\n","ax1.imshow(X_test[0,:,:,0], cmap=pl.get_cmap('Greys'), interpolation='nearest')\n","ax1.set_title('Input image')\n","ax1.set_adjustable('box')\n","\n","ax2.axis('off')\n","ax2.imshow(hog_image_rescaled, cmap=pl.get_cmap('Greys'), interpolation='nearest')\n","ax2.set_title('Histogram of Oriented Gradients')\n","ax1.set_adjustable('box')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Yv-ep9ybag4"},"source":["Compute the HOGs for the train, the validation and the test datasets\n","\n","> Indented block\n","\n"]},{"cell_type":"code","metadata":{"id":"y8Q3cdrebag6"},"source":["X_train_hog = np.zeros((X_train.shape[0], hog_size))\n","X_val_hog = np.zeros((X_val.shape[0], hog_size))\n","X_test_hog = np.zeros((X_test.shape[0], hog_size))\n","\n","for i in np.arange(X_train_hog.shape[0]):\n","    X_train_hog[i,:] = hog(X_train[i,:,:,0], orientations=n_orientations, pixels_per_cell=(pix_p_cell, pix_p_cell), cells_per_block=(1, 1), visualize=False)\n","print('X_train done')\n","\n","for i in np.arange(X_val_hog.shape[0]):\n","    X_val_hog[i,:] = hog(X_val[i,:,:,0], orientations=n_orientations, pixels_per_cell=(pix_p_cell, pix_p_cell), cells_per_block=(1, 1), visualize=False)\n","print('X_val done')\n","\n","for i in np.arange(X_test_hog.shape[0]):\n","    X_test_hog[i,:] = hog(X_test[i,:,:,0], orientations=n_orientations, pixels_per_cell=(pix_p_cell, pix_p_cell), cells_per_block=(1, 1), visualize=False)\n","print('X_test done')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kos0aTYmbahU"},"source":["Create the MLP"]},{"cell_type":"code","metadata":{"id":"NnawtqztbahV"},"source":["model = Sequential()\n","model.add(Dense(300, input_shape=(hog_size,), activation='relu'))\n","#model.add(Dropout(0.5))\n","model.add(Dense(n_classes, activation='softmax'))\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EGeviXzJbahY"},"source":["Define some constants and train the MLP. In order to perform the model selection process, you train each model with the train dataset and evaluate it with the validation dataset. The test set remains unseen."]},{"cell_type":"code","metadata":{"id":"laKFY8vcbahZ"},"source":["batch_size = 128\n","n_epoch = 5\n","\n","model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n","history = model.fit(X_train_hog, Y_train,\n","                    batch_size=batch_size, epochs=n_epoch,\n","                    verbose=1, validation_data=(X_val_hog, Y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aezTZuCMbahf"},"source":["Show the performance of the model. By observing the performance of your models on the validation set, you will choose a set of hyperparameters for your final model. The test set is still not used at this point."]},{"cell_type":"code","metadata":{"id":"u6E66I52bahg"},"source":["pl.plot(history.history['loss'], label='Training')\n","pl.plot(history.history['val_loss'], label='Validation')\n","pl.legend()\n","pl.grid()\n","\n","score = model.evaluate(X_val_hog, Y_val, verbose=0)\n","print('Validation score:', score[0])\n","print('Validation accuracy:', score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that you have chosen your final model, you can finally evaluate its performance using the test set. It is important that the test set remains hidden for your model until this last step in order to have an unbiased estimate of the performance. Therefore, you should not run the following cell until you have chosen your final model."],"metadata":{"id":"MbgsBu3-_-UR"}},{"cell_type":"code","source":["final_score = model.evaluate(X_test_hog, Y_test, verbose=0)\n","print('FINAL RESULTS:')\n","print('Test score:', final_score[0])\n","print('Test accuracy:', final_score[1])"],"metadata":{"id":"wNhgXsFMAD-_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"axdxLHYMbahu"},"source":["Confusion matrix"]},{"cell_type":"code","source":[],"metadata":{"id":"Wt0P_mVSCwfW"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mhm1wjNfbahv"},"source":["pred = model.predict(X_test_hog)\n","pred = np.argmax(pred, axis=-1)\n","me.confusion_matrix(y_test, pred)"],"execution_count":null,"outputs":[]}]}