{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "747b5d41",
   "metadata": {},
   "source": [
    "# Ex2 - Review questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60f6082",
   "metadata": {},
   "source": [
    "### a) What are the two fundamental ideas a SVM are built on ? Summarize them with your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d929674",
   "metadata": {},
   "source": [
    "\n",
    "1. **Maximiser la margin** : Le but principale du SVM est de trouver un plan/ligne qui permet de séparer des classes le mieux possible et donc en essayant de maximiser la \"margin\". Cette margin est la distance entre le plan/ligne est le point le plus proche de chaque classe. En maximisant cela, le SVM est capable de renforcé le modèle en comparaison avec la logistic regression qu'on a vu dans le travail précédant. Il aura notamment une plus grande robustesse quand il faudra classifier des nouvelles données que le modèle n'a pas encore vu. \n",
    "2. **Kernel Trick** :  Le deuxième point concernant l'utilisation d'un kernel specifique. Cette technique est utilisé pour traiter les relations non-linéaires des données et donc de pouvoir les séparés de manière plus simple. Le kernel permet au SVM de mapper les données (polynomial) dans un espace de dimensions supérieurs ou la séparation sera plus simple a faire grâce notamment à un plan. Ce \"trick\" est essentielle pour les modèles SVM car ils permettent de modèliser les relations complexes sans augmenter de manière significative la complexité computationnelle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266203b9",
   "metadata": {},
   "source": [
    "### b) With the hinge loss, training points can fall into three cases. Re-explain these cases with your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb3fe5",
   "metadata": {},
   "source": [
    "1. **Points correctement classifiés** : Si un \"training point\" est correctement classé et se trouve en dehors de la margin (c'est-à-dire du bon côté de la ligne), aucune perte ne lui est associée. Ces points sont essentiels pour définir la limite de décision mais ne contribuent pas à la loss function.\n",
    "2. **Points dans la margin** : Si un \"training point\" est correctement classé mais se trouve à l'intérieur de la margin, il est associé à une perte non nulle. Bien que le point se trouve du bon côté de la frontière de décision, il est trop proche de celle-ci et cette proximité entraîne une pénalité. Le SVM vise à maximiser la marge, de sorte que les points situés à l'intérieur de la marge subissent une perte, ce qui encourage le SVM à trouver une frontière de décision avec une marge plus importante.\n",
    "3. **Points mal-classés** :  Si un point est mal classé (c'est-à-dire s'il se trouve du mauvais côté de la ligne de séparation), il est associé à une non-zero loss. Les points mal classés sont cruciaux car ils contribuent de manière significative à la loss function. Les SVM visent à minimiser la perte associée aux points mal classés, en encourageant le modèle à ajuster la frontière de décision afin de classer correctement ces mêmes points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18438801",
   "metadata": {},
   "source": [
    "### c) What are the two implementations of SVMs available in SciKit Learn ? Which one would you take if you have a system that needs to incorporate incremental learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797101d2",
   "metadata": {},
   "source": [
    "1. **SVC (Support Vector Classification):** Cette classe dans scikit-learn est utilisé (comme toutes les autres) pour faire de la classification. Elle peut gérer des classifications linéaires et non-linéaires avec l'utilisation de différents kernels. \n",
    "2. **LinearSVC (Linear Support Vector Classification)** : Classe similaire à SVC, la différence est que celle est spécifiquement utilisé pour faire de la classification linéaire et elle est optimisée pour faire cela. L'utilisation de kernel linéaire sur beaucoup de données (features) fait que le cette fonctionne très bien. \n",
    "\n",
    "Pour **incremental learning**, nous préférons **LinearSVC** surtout pour les grands ensembles de données où nous devons optimiser la vitesse de l'entraînement et l'efficacité.  LinearSVC est spécialement conçu pour les \"linear kernel\" et est généralement plus rapide et plus efficace en termes de mémoire que SVC avec un noyau linéaire. Il convient parfaitement aux grands ensembles de données et est capable de gérer efficacement les scénarios d'apprentissage incrémentiel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd848f8",
   "metadata": {},
   "source": [
    "### d) A SVM can classify between 2 classes. Cite and explain in your own words the 2 strategies we have to build a multi-class (with K classes) system with SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2fefe",
   "metadata": {},
   "source": [
    "2 stratégies sont possibles: \n",
    "\n",
    "1. **One-vs-One** : Dans cette stratégie, un classifieur binaire est entraîné pour chaque paire possible de classes. Par exemple, si nous avons K classes, nous créerons K(K-1)/2 classifieurs. Lors de la phase de prédiction, chaque classifieur binaire donne son vote pour une classe particulière. La classe qui reçoit le plus grand nombre de votes est alors sélectionnée comme la prédiction finale. Cette méthode est simple et facile à mettre en œuvre, mais elle peut être coûteuse en termes de temps d'entraînement, surtout lorsque le nombre de classes est élevé.\n",
    "2. **One-vs-All** : Comme pour la stratégie d'avant classifieur binaire est entraîné pour chaque classe, en considérant cette classe comme la classe positive et toutes les autres classes comme la classe négative. Par exemple, si nous avons K classes, nous entraînerons K classifieurs. Pendant la phase de prédiction, chaque classifieur donne une décision binaire (appartient ou n'appartient pas à la classe). La classe pour laquelle le classifieur donne la prédiction positive la plus confiante est sélectionnée comme la classe prédite. Cette méthode est généralement plus rapide à entraîner que la stratégie un-contre-un, surtout lorsque le nombre de classes est élevé. Elle est également plus adaptée aux ensembles de données déséquilibrés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c6821",
   "metadata": {},
   "source": [
    "### e) Are the strategies of point d) equal in terms of cpu ? (elaborate your answer considering training and testing times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca341ee1",
   "metadata": {},
   "source": [
    "Source : https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\n",
    "\n",
    "1. **One-vs-One** : Lorsque la stratégie One-vs-One est utilisée, le nombre de classifieurs à entraîner est proportionnel au carré du nombre de classes (K(K-1)/2). Cela signifie que pour un grand nombre de classes, le nombre de classifieurs peut devenir assez élevé très rapidement. L'entraînement de chaque classifieur nécessite des ressources CPU significatives, et cela s'additionne pour chaque paire de classes à considérer. Le temps d'entraînement total peut donc être assez élevé, en particulier dans le cas de grands ensembles de données.\n",
    "2. **One-vs-All** : Avec la stratégie One-vs-All, le nombre de classifieurs à entraîner est égal au nombre de classes (K). Chaque classifieur est entraîné pour séparer une classe spécifique du reste des classes. Comparé à One-vs-One, cette approche a généralement besoin de moins de classifieurs à entraîner. En conséquence, le temps d'entraînement total peut être moindre par rapport à la stratégie One-vs-One, en particulier pour un grand nombre de classes.\n",
    "\n",
    "En ce qui concerne le temps de performance : \n",
    "\n",
    "La stratégie One-vs-All a tendance à être plus rapide. Lors de la phase de test, chaque classifieur donne une décision binaire, et la classe prédite est celle avec le score le plus élevé. Étant donné que chaque classifieur est spécifique à une classe, la prédiction pour un nouvel exemple peut être obtenue en utilisant simplement tous les classifieurs et en choisissant celui avec le score le plus élevé. Dans le cas de One-vs-One, il faut également parcourir tous les classifieurs pour chaque exemple, ce qui peut entraîner un temps de test plus long, surtout si le nombre de classifieurs est élevé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ede30",
   "metadata": {},
   "source": [
    "### f) Describe a machine learning task for which SVM would be a better choice than any of the algorithms previously studied. Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18048e25",
   "metadata": {},
   "source": [
    "\n",
    "sance de la parole implique la conversion de signaux audio en texte. Les données audio sont complexes et peuvent avoir des caractéristiques temporelles et fréquentielles variées. Les SVM sont capables de traiter des données complexes et peuvent trouver des motifs dans des espaces de grande dimension, ce qui en fait un choix adapté pour ce type de tâche.\n",
    "2. **Capacité à Gérer les Données de Grande Dimension** : Les signaux audio peuvent être représentés sous forme de vecteurs avec des milliers de caractéristiques, en fonction des échantillonnages par seconde. Les SVM sont efficaces pour travailler dans des espaces de grande dimension, et ils peuvent gérer des ensembles de données comportant un grand nombre de caractéristiques sans surajustement, ce qui est un avantage essentiel dans la reconnaissance de la parole. Aucun changement de la voix est nécessaire.\n",
    "\n",
    "3. **Robustesse Face au Bruit** : Les systèmes audio peuvent être sujets à des bruits de fond et d'autres interférences qui pourrait compliquer la vie du modèle pour claissifier des données. Les SVM ont la capacité de généraliser à partir de données bruitées, ce qui signifie qu'ils peuvent maintenir leur performance même en présence de perturbations, ce qui est une caractéristique essentielle dans les applications de reconnaissance de la parole dans des environnements réels.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
