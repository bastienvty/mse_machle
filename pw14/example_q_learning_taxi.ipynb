{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rKQOC4d5mL-J"},"outputs":[],"source":["import gym\n","from IPython.display import clear_output\n","from time import sleep\n","import numpy as np\n","from matplotlib import pyplot as pl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAfztMnhmL-L"},"outputs":[],"source":["env = gym.make(\"Taxi-v3\", render_mode=\"ansi\").env\n","env.reset()\n","env.render();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQ47oYGMmL-M"},"outputs":[],"source":["def print_frames(frames):\n","    for i, frame in enumerate(frames):\n","        clear_output(wait=True)\n","        print(frame['frame'][0])\n","        print(f'Timestep: {i + 1}')\n","        print(f\"State: {frame['state']}\")\n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        sleep(.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Irwxf6-3mL-N"},"outputs":[],"source":["actions_dict = {0:'south',\n","                1:'north',\n","                2:'east',\n","                3:'west',\n","                4:'pickup',\n","                5:'dropoff'}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gCTXSLVmL-N"},"outputs":[],"source":["q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","q_table.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-v5sUlrLmL-O"},"outputs":[],"source":["%%time\n","\n","print('Training the agent')\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","for i in range(1, 100001):\n","    state = env.reset()\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","\n","    while not done:\n","        if np.random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state,:]) # Exploit learned values\n","\n","        next_state, reward, done, info = env.step(action)\n","\n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state,:])\n","\n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","        q_table[state, action] = new_value\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        state = next_state\n","        epochs += 1\n","\n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f'Episode: {i}')\n","\n","print('Training finished.\\n')\n","print(f'Penalties: {penalties}')"]},{"cell_type":"code","source":["# Find the index of the best action for each state\n","best_actions = np.argmax(q_table, axis=1)\n","\n","# Create a new array representing the best actions for each state\n","best_actions_array = np.zeros_like(q_table)\n","best_actions_array[np.arange(len(best_actions)), best_actions] = 1\n","\n","# Plot the Q-table\n","pl.figure(figsize=(16, 8))\n","\n","# Plot Q-table\n","pl.subplot(2, 1, 1)\n","pl.imshow(q_table.T, interpolation='nearest', aspect=10)\n","pl.yticks(range(len(actions_dict)), [actions_dict[k] for k in range(len(actions_dict))])\n","pl.title('Q table')\n","pl.xlabel('States')\n","pl.ylabel('Actions')\n","\n","# Plot best actions\n","pl.subplot(2, 1, 2)\n","pl.imshow(best_actions_array.T, interpolation='nearest', aspect=10)\n","pl.yticks(range(len(actions_dict)), [actions_dict[k] for k in range(len(actions_dict))])\n","pl.title('Best Actions')\n","pl.xlabel('States')\n","pl.ylabel('Actions')\n","\n","pl.tight_layout()\n","pl.show()\n"],"metadata":{"id":"8mC7KMlImMfp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Bp0gDrrmL-Q"},"outputs":[],"source":["state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n","print(\"State:\", state)\n","\n","env.s = state\n","env.render()\n","\n","print(q_table[state])\n","print(f'Selected action: {actions_dict[np.argmax(q_table[state])]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m0RtF2pymL-R"},"outputs":[],"source":["print(\"Evaluate agent's performance after Q-learning\")\n","\n","total_epochs, total_penalties = 0, 0\n","episodes = 100\n","\n","for _ in range(episodes):\n","    state = env.reset()\n","    epochs, penalties, reward = 0, 0, 0\n","\n","    done = False\n","\n","    while not done:\n","        action = np.argmax(q_table[state,:])\n","        state, reward, done, info = env.step(action)\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        epochs += 1\n","\n","    total_penalties += penalties\n","    total_epochs += epochs\n","\n","print(f'Results after {episodes} episodes:')\n","print(f'Average timesteps per episode: {total_epochs / episodes}')\n","print(f'Average penalties per episode: {total_penalties / episodes}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2P3q4GimL-S"},"outputs":[],"source":["print(\"Visualizing an individual episode\")\n","\n","state = env.reset()\n","epochs, penalties, reward = 0, 0, 0\n","\n","done = False\n","\n","while not done:\n","    action = np.argmax(q_table[state,:])\n","    state, reward, done, info = env.step(action)\n","\n","    if reward == -10:\n","        penalties += 1\n","\n","    epochs += 1\n","\n","    clear_output(wait=True)\n","    env.render()\n","    sleep(.2)\n","\n","print(f'Timesteps: {epochs}')\n","print(f'Penalties: {penalties}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1unVgGwmL-T"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}